{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install qalsadi\n",
    "from qalsadi.lemmatizer import Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_lemmatize(word):\n",
    "    lemmatizer = Lemmatizer()\n",
    "    return lemmatizer.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_docs = [\"وقبر حربٍ بمكان قفرٍ\", \"وليس قرب قبر حربٍ قبرٌ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'قبر': 1, 'حرب': 2, 'مكان': 3, 'قفر': 4, 'ليس': 5, 'قرب': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "c = 0\n",
    "for doc in ar_docs:\n",
    "    words = doc.split(' ')\n",
    "    for w in words:\n",
    "        w = arabic_lemmatize(w)\n",
    "        if w not in vocab:\n",
    "            c += 1\n",
    "            vocab[w] = c\n",
    "            \n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_vector(text):\n",
    "    encodedVector = []\n",
    "    text_tokenized = text.split(' ')\n",
    "    for w in text_tokenized:\n",
    "        w = arabic_lemmatize(w)\n",
    "        tmp = [0] * len(vocab)\n",
    "        if w in vocab:\n",
    "            tmp[vocab[w] - 1] = 1\n",
    "        encodedVector.append(tmp)\n",
    "    return encodedVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وقبر حربٍ بمكان قفرٍ\n",
      "[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
      "وليس قرب قبر حربٍ قبرٌ\n",
      "[[0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for doc in ar_docs:\n",
    "    print(doc)\n",
    "    print( get_onehot_vector(doc) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot Encoding Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'bites', 'man', 'man', 'bites', 'dog', 'dog', 'eats', 'meat', 'man', 'eats', 'food']\n",
      "[['dog', 'bites', 'man'], ['man', 'bites', 'dog'], ['dog', 'eats', 'meat'], ['man', 'eats', 'food']]\n"
     ]
    }
   ],
   "source": [
    "d = ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
    "\n",
    "words = []\n",
    "sentences = []\n",
    "for s in d:\n",
    "    s = s.split()\n",
    "    words += s\n",
    "    sentences.append(s)\n",
    "\n",
    "print(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 4 0 1 1 2 5 4 2 3]\n",
      "[[1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 0.]]\n",
      "  (0, 0)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (0, 6)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (2, 7)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (3, 5)\t1.0\n"
     ]
    }
   ],
   "source": [
    "encodedData = LabelEncoder().fit_transform(words)\n",
    "print(encodedData)\n",
    "\n",
    "onehotEncoded = OneHotEncoder().fit_transform(sentences)\n",
    "onehotEncodedArr = onehotEncoded.toarray()\n",
    "print(onehotEncodedArr)\n",
    "print(onehotEncoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
