{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # Natural Language Toolkit that provide tools to work with human language text.\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: As a simple rule, we can do sentence segmentation by breaking up text into sentences at the appearance of full stops and question marks.\n",
      "2: However, there may be abbreviations, forms of addresses (Dr., Mr., etc.\n",
      "3: ), or ellipses (...) that may break the simple rule.\n",
      "4: Thankfully, we don’t have to worry about how to solve these issues, as most NLP libraries come with some form of sentence and word splitting implemented.\n",
      "5: A commonly used library is Natural Language Tool Kit (NLTK) [30].\n",
      "6: The code example below shows how to use a sentence and word splitter from NLTK and uses the first paragraph of this chapter as input:\n"
     ]
    }
   ],
   "source": [
    "text = \"As a simple rule, we can do sentence segmentation by breaking up text into sentences at the appearance of full stops and question marks. However, there may be abbreviations, forms of addresses (Dr., Mr., etc.), or ellipses (...) that may break the simple rule. Thankfully, we don’t have to worry about how to solve these issues, as most NLP libraries come with some form of sentence and word splitting implemented. A commonly used library is Natural Language Tool Kit (NLTK) [30]. The code example below shows how to use a sentence and word splitter from NLTK and uses the first paragraph of this chapter as input:\"\n",
    "sentences = sent_tokenize(text)\n",
    "i=1\n",
    "for sentence in sentences:\n",
    "    print(str(i)+\":\",sentence)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seperates when there is a full stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['As', 'a', 'simple', 'rule', ',', 'we', 'can', 'do', 'sentence', 'segmentation', 'by', 'breaking', 'up', 'text', 'into', 'sentences', 'at', 'the', 'appearance', 'of', 'full', 'stops', 'and', 'question', 'marks', '.']\n",
      "['However', ',', 'there', 'may', 'be', 'abbreviations', ',', 'forms', 'of', 'addresses', '(', 'Dr.', ',', 'Mr.', ',', 'etc', '.']\n",
      "[')', ',', 'or', 'ellipses', '(', '...', ')', 'that', 'may', 'break', 'the', 'simple', 'rule', '.']\n",
      "['Thankfully', ',', 'we', 'don', '’', 't', 'have', 'to', 'worry', 'about', 'how', 'to', 'solve', 'these', 'issues', ',', 'as', 'most', 'NLP', 'libraries', 'come', 'with', 'some', 'form', 'of', 'sentence', 'and', 'word', 'splitting', 'implemented', '.']\n",
      "['A', 'commonly', 'used', 'library', 'is', 'Natural', 'Language', 'Tool', 'Kit', '(', 'NLTK', ')', '[', '30', ']', '.']\n",
      "['The', 'code', 'example', 'below', 'shows', 'how', 'to', 'use', 'a', 'sentence', 'and', 'word', 'splitter', 'from', 'NLTK', 'and', 'uses', 'the', 'first', 'paragraph', 'of', 'this', 'chapter', 'as', 'input', ':']\n",
      "\n",
      "all_words: ['As', 'a', 'simple', 'rule', ',', 'we', 'can', 'do', 'sentence', 'segmentation', 'by', 'breaking', 'up', 'text', 'into', 'sentences', 'at', 'the', 'appearance', 'of', 'full', 'stops', 'and', 'question', 'marks', '.', 'However', ',', 'there', 'may', 'be', 'abbreviations', ',', 'forms', 'of', 'addresses', '(', 'Dr.', ',', 'Mr.', ',', 'etc', '.', ')', ',', 'or', 'ellipses', '(', '...', ')', 'that', 'may', 'break', 'the', 'simple', 'rule', '.', 'Thankfully', ',', 'we', 'don', '’', 't', 'have', 'to', 'worry', 'about', 'how', 'to', 'solve', 'these', 'issues', ',', 'as', 'most', 'NLP', 'libraries', 'come', 'with', 'some', 'form', 'of', 'sentence', 'and', 'word', 'splitting', 'implemented', '.', 'A', 'commonly', 'used', 'library', 'is', 'Natural', 'Language', 'Tool', 'Kit', '(', 'NLTK', ')', '[', '30', ']', '.', 'The', 'code', 'example', 'below', 'shows', 'how', 'to', 'use', 'a', 'sentence', 'and', 'word', 'splitter', 'from', 'NLTK', 'and', 'uses', 'the', 'first', 'paragraph', 'of', 'this', 'chapter', 'as', 'input', ':']\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences))\n",
    "for sentence in sentences:\n",
    "    print(word_tokenize(sentence))\n",
    "    \n",
    "print(\"\\nall_words:\",word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: الله يهدي قلبك.\n",
      "2: القرآن عزيز لو مأقبلتش عليه بقلبك وكل جوارحك هيبعد عليك اللي بتتمناه، والقرآن مابيعطلش ومابيأخرش حد بل هو هدى ونور وشفاء وبصيرة وبيان وقوة.\n",
      "3: اعترافك بتقصيرك وأنك تقدر لكن نفسك تغلبك أفضل من مخادعات ضيق الوقت وضغط الحياة .حفظ القرآن ثوابه عظيم لكنه فرض كفاية، (ماقدرش ألومك لو قصرت لأن فيه غيرك بيحفظ)، فلو حابب تيجي تقرأ بس تعالى : دائمًا مرحب إن شاء الله وأكيد أفضل بكثير من الانقطاع\n"
     ]
    }
   ],
   "source": [
    "text = 'الله يهدي قلبك. القرآن عزيز لو مأقبلتش عليه بقلبك وكل جوارحك هيبعد عليك اللي بتتمناه، والقرآن مابيعطلش ومابيأخرش حد بل هو هدى ونور وشفاء وبصيرة وبيان وقوة. اعترافك بتقصيرك وأنك تقدر لكن نفسك تغلبك أفضل من مخادعات ضيق الوقت وضغط الحياة .حفظ القرآن ثوابه عظيم لكنه فرض كفاية، (ماقدرش ألومك لو قصرت لأن فيه غيرك بيحفظ)، فلو حابب تيجي تقرأ بس تعالى : دائمًا مرحب إن شاء الله وأكيد أفضل بكثير من الانقطاع'\n",
    "sentences = sent_tokenize(text)\n",
    "i=1\n",
    "for sentence in sentences:\n",
    "    print(str(i)+\":\",sentence)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['الله', 'يهدي', 'قلبك', '.']\n",
      "['القرآن', 'عزيز', 'لو', 'مأقبلتش', 'عليه', 'بقلبك', 'وكل', 'جوارحك', 'هيبعد', 'عليك', 'اللي', 'بتتمناه،', 'والقرآن', 'مابيعطلش', 'ومابيأخرش', 'حد', 'بل', 'هو', 'هدى', 'ونور', 'وشفاء', 'وبصيرة', 'وبيان', 'وقوة', '.']\n",
      "['اعترافك', 'بتقصيرك', 'وأنك', 'تقدر', 'لكن', 'نفسك', 'تغلبك', 'أفضل', 'من', 'مخادعات', 'ضيق', 'الوقت', 'وضغط', 'الحياة', '.حفظ', 'القرآن', 'ثوابه', 'عظيم', 'لكنه', 'فرض', 'كفاية،', '(', 'ماقدرش', 'ألومك', 'لو', 'قصرت', 'لأن', 'فيه', 'غيرك', 'بيحفظ', ')', '،', 'فلو', 'حابب', 'تيجي', 'تقرأ', 'بس', 'تعالى', ':', 'دائمًا', 'مرحب', 'إن', 'شاء', 'الله', 'وأكيد', 'أفضل', 'بكثير', 'من', 'الانقطاع']\n",
      "\n",
      "all_words: ['الله', 'يهدي', 'قلبك', '.', 'القرآن', 'عزيز', 'لو', 'مأقبلتش', 'عليه', 'بقلبك', 'وكل', 'جوارحك', 'هيبعد', 'عليك', 'اللي', 'بتتمناه،', 'والقرآن', 'مابيعطلش', 'ومابيأخرش', 'حد', 'بل', 'هو', 'هدى', 'ونور', 'وشفاء', 'وبصيرة', 'وبيان', 'وقوة', '.', 'اعترافك', 'بتقصيرك', 'وأنك', 'تقدر', 'لكن', 'نفسك', 'تغلبك', 'أفضل', 'من', 'مخادعات', 'ضيق', 'الوقت', 'وضغط', 'الحياة', '.حفظ', 'القرآن', 'ثوابه', 'عظيم', 'لكنه', 'فرض', 'كفاية،', '(', 'ماقدرش', 'ألومك', 'لو', 'قصرت', 'لأن', 'فيه', 'غيرك', 'بيحفظ', ')', '،', 'فلو', 'حابب', 'تيجي', 'تقرأ', 'بس', 'تعالى', ':', 'دائمًا', 'مرحب', 'إن', 'شاء', 'الله', 'وأكيد', 'أفضل', 'بكثير', 'من', 'الانقطاع']\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences))\n",
    "for sentence in sentences:\n",
    "    print(word_tokenize(sentence))\n",
    "    \n",
    "print(\"\\nall_words:\",word_tokenize(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    mystopwords.update(set(stopwords.words(\"arabic\")))\n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token.lower() for token in tokens if token not in mystopwords and not token.isdigit() and token not in punctuation]\n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'love', 'dr.', 'alattar', 'afify', 'ashraf', 'galal', 'a', 'game', 'thrones', 'play'], ['stupid', 'bro'], ['بحب', 'العربي', 'جدًا', 'أعماق', 'قلبي']]\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"I love Dr. Alattar and Afify and Ashraf and Galal. A game of thrones play.\"\n",
    "txt2 = \"that was very stupid bro\"\n",
    "txt3 = \"بحب العربي جدًا من أعماق قلبي\"\n",
    "txts = [txt1,txt2,txt3]\n",
    "print(preprocess_corpus(txts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.isri import ISRIStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car congratul\n",
      "حرك\n",
      "قلب\n",
      "علم\n"
     ]
    }
   ],
   "source": [
    "eng_stemmer = PorterStemmer()\n",
    "eng_words = ['cars','Congratulations']\n",
    "print(eng_stemmer.stem(eng_words[0]),eng_stemmer.stem(eng_words[1]))\n",
    "\n",
    "arab_stemmer = ISRIStemmer()\n",
    "arab_words = [\"حركات\",\"قلبي\",\"إعلاميون\"]\n",
    "for w in arab_words:\n",
    "    print(arab_stemmer.stem(w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      " \n",
      "cobblers\n",
      "ants\n",
      "women\n",
      "boys\n",
      "need\n",
      "find\n",
      "binaries\n",
      "hobbies\n",
      "buss\n",
      "wolves\n",
      "better\n",
      " \n",
      "cobbler\n",
      "ant\n",
      "woman\n",
      "boy\n",
      "need\n",
      "find\n",
      "binary\n",
      "hobby\n",
      "bus\n",
      "wolf\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "eng_word_adj = 'better'\n",
    "print(lemmatizer.lemmatize(eng_word_adj,pos='a')) # a refers to adj\n",
    "print(' ')\n",
    "eng_words = 'cobblers ants women boys needs finds binaries hobbies busses wolves better'.split()\n",
    "for w in eng_words:\n",
    "    print(lemmatizer.lemmatize(w,pos='v')) # v refers to verb\n",
    "print(' ')\n",
    "for w in eng_words:\n",
    "    print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install qalsadi\n",
    "from qalsadi.lemmatizer import Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "احتاج\n",
      "\n",
      "احتاج\n"
     ]
    }
   ],
   "source": [
    "arab_lemmatizer = Lemmatizer()\n",
    "print(arab_lemmatizer.lemmatize(\"يحتاج\"))\n",
    "print(arab_lemmatizer.lemmatize(\"يحتاج\",pos='n'))\n",
    "print(arab_lemmatizer.lemmatize(\"يحتاج\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['هل', 'احتاج', 'إلى', 'ترجمة', 'كي', 'تف', 'خطاب', 'ملك', '؟', 'لغة', '\"', 'كلاسيكي', '\"(', 'فصحى', ')', 'موجود', 'في', 'كل', 'لغة', 'ذلك', 'لغة', '\"', 'دارج', '\"..', 'فرنسة', 'التي', 'درس', 'في', 'مدرس', 'ليست', 'فرنسة', 'التي', 'استخدم', 'ناس', 'في', 'شوارع', 'باريس', '..', 'ملك', 'بريطاني', 'لا', 'خطب', 'بلغة', 'شوارع', 'أدان', '..', 'كل', 'مقام', 'مقال'] \n",
      "\n",
      "[('هل', 'stopword'), ('احتاج', 'verb'), ('إلى', 'stopword'), ('ترجمة', 'noun'), ('كي', 'stopword'), ('تف', 'noun'), ('خطاب', 'noun'), ('ملك', 'noun'), '؟', ('لغة', 'noun'), '\"', ('كلاسيكي', 'noun'), '\"(', ('فصحى', 'noun'), ')', ('موجود', 'noun'), ('في', 'stopword'), ('كل', 'stopword'), ('لغة', 'noun'), ('ذلك', 'stopword'), ('لغة', 'noun'), '\"', ('دارج', 'noun'), '\"..', ('فرنسة', 'noun'), ('التي', 'stopword'), ('درس', 'verb'), ('في', 'stopword'), ('مدرس', 'noun'), ('ليست', 'stopword'), ('فرنسة', 'noun'), ('التي', 'stopword'), ('استخدم', 'verb'), ('ناس', 'noun'), ('في', 'stopword'), ('شوارع', 'noun'), ('باريس', 'all'), '..', ('ملك', 'noun'), ('بريطاني', 'noun'), ('لا', 'stopword'), ('خطب', 'verb'), ('بلغة', 'noun'), ('شوارع', 'noun'), ('أدان', 'verb'), '..', ('كل', 'stopword'), ('مقام', 'noun'), ('مقال', 'noun')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "arab_text = \"\"\"هل تحتاج إلى ترجمة كي تفهم خطاب الملك؟ اللغة \"الكلاسيكية\" (الفصحى) موجودة في كل اللغات وكذلك اللغة \"الدارجة\" .. الفرنسية التي ندرس في المدرسة ليست الفرنسية التي يستخدمها الناس في شوارع باريس .. وملكة بريطانيا لا تخطب بلغة شوارع لندن .. لكل مقام مقال\"\"\"\n",
    "lemmas = arab_lemmatizer.lemmatize_text(arab_text)\n",
    "print(lemmas,'\\n')\n",
    "lemmas_pos = arab_lemmatizer.lemmatize_text(arab_text,return_pos=True)\n",
    "print(lemmas_pos,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization using spaCy\n",
    "# %pip install spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cobblers cobbler\n",
      "ants ant\n",
      "women woman\n",
      "boys boy\n",
      "needs need\n",
      "finds find\n",
      "binaries binary\n",
      "hobbies hobbie\n",
      "busses bus\n",
      "wolves wolf\n",
      "better well\n"
     ]
    }
   ],
   "source": [
    "sp = spacy.load('en_core_web_sm')\n",
    "words = 'cobblers ants women boys needs finds binaries hobbies busses wolves better'.split()\n",
    "for w in words:\n",
    "    token = sp(w)\n",
    "    for word in token:\n",
    "        print(word.text,word.lemma_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Charles PROPN True False\n",
      "Spencer PROPN True False\n",
      "Chaplin PROPN True False\n",
      "was AUX True True\n",
      "born VERB True False\n",
      "on ADP True True\n",
      "16 NUM False False\n",
      "April PROPN True False\n",
      "1889 NUM False False\n",
      "to ADP True True\n",
      "Hannah PROPN True False\n",
      "Chaplin PROPN True False\n",
      "( PUNCT False False\n",
      "born VERB True False\n",
      "Hannah PROPN True False\n",
      "Harriet PROPN True False\n",
      "Pedlingham PROPN True False\n",
      "Hill PROPN True False\n",
      ") PUNCT False False\n",
      "and CCONJ True True\n",
      "Charles PROPN True False\n",
      "Chaplin PROPN True False\n",
      "Sr PROPN True False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Charles Spencer Chaplin was born on 16 April 1889 to Hannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr')\n",
    "print(type(doc))\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.is_alpha,token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53d50ed1839d2292cfd39f0644d59e889da85062f836d8db82f8d3a293c0c3a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
